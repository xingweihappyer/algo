

import torch
import numpy as np
import torch.autograd as autograd
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.autograd import Variable
corpus = [
    'he is a king',
    'she is a queen',
    'he is a man',
    'she is a woman',
    'warsaw is poland capital',
    'berlin is germany capital',
    'paris is france capital',
]

def tokenize_corpus(corpus):
    tokens=[x.split() for x in corpus]
    return tokens


tokenized_corpus=tokenize_corpus(corpus)
print(tokenized_corpus)

vocabulary=[]
for sentence in tokenized_corpus:
    for token in sentence:
        if token not in vocabulary:
            vocabulary.append(token)
word2idx={w:idx for (idx,w) in enumerate(vocabulary)}
idx2word={idx:w for (idx,w) in enumerate(vocabulary)}

vocabulary_size=len(vocabulary)

print(idx2word)
window_size=2
idx_pairs=[]
for sentence in tokenized_corpus:
    indices=[word2idx[word] for word in sentence]
    for center_word_pos in range(len(indices)):
        for w in range(-window_size,window_size+1):
            context_word_pos=center_word_pos+w
            if context_word_pos<0 or context_word_pos>= len(indices) or center_word_pos==context_word_pos:
                continue
            context_word_idx=indices[context_word_pos]
            idx_pairs.append([indices[center_word_pos],context_word_idx])
idx_pairs=np.array(idx_pairs)
print(idx_pairs)

# 查看idx_pair索引对应的单词
for i in idx_pairs :
    print(idx2word[i[0]],idx2word[i[1]])


def get_input_layer(word_idx):
    x=torch.zeros(vocabulary_size).float()
    x[word_idx]=1.0
    return x

embedding_size=128
W1=Variable(torch.randn(embedding_size,vocabulary_size).float(),requires_grad=True)
W2 = Variable(torch.randn(vocabulary_size, embedding_size).float(), requires_grad=True)
num_epochs = 100
learning_rate = 0.001
for epo in range(num_epochs):
    loss_val=0
    for data,target in idx_pairs:
        x= Variable(get_input_layer(data)).float()
        y_true=Variable(torch.from_numpy(np.array([target])).long())
        z1=torch.matmul(W1,x)
        z2=torch.matmul(W2,z1)
        log_softmax=F.log_softmax(z2,dim=0)
        loss = F.nll_loss(log_softmax.view(1,-1), y_true)
        loss_val += loss.item()
        loss.backward()
        W1.data -= learning_rate * W1.grad.data
        W2.data -= learning_rate * W2.grad.data

        W1.grad.data.zero_()
        W2.grad.data.zero_()
    if epo % 10 == 0:
        print(f'Loss at epo {epo}: {loss_val/len(idx_pairs)}')

"""
提取向量
现在我们训练了一个网络，最后一件事就是提取每个单词的向量，这里有三个可能的方式
- 使用W1的v向量
- 使用W2的u向量
- 使用u和v的平均
你可以自己思考什么时候用哪个
"""

print('W1.shape : ',W1.shape)
print('W2.shape : ',W2.shape)

